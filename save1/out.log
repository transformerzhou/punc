2021-07-11 13:34:01,415 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2021-07-11 13:34:01,415 - INFO - allennlp.common.params - dataset_reader.type = data_reader
2021-07-11 13:34:01,416 - INFO - allennlp.common.params - dataset_reader.max_instances = None
2021-07-11 13:34:01,416 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False
2021-07-11 13:34:01,416 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False
2021-07-11 13:34:01,416 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer
2021-07-11 13:34:01,416 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = hfl/chinese-roberta-wwm-ext-large
2021-07-11 13:34:01,416 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True
2021-07-11 13:34:01,417 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = None
2021-07-11 13:34:01,417 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None
2021-07-11 13:34:09,625 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = pretrained_transformer
2021-07-11 13:34:09,628 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.token_min_padding_length = 0
2021-07-11 13:34:09,628 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.model_name = hfl/chinese-roberta-wwm-ext-large
2021-07-11 13:34:09,629 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.namespace = tags
2021-07-11 13:34:09,629 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_length = None
2021-07-11 13:34:09,629 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.tokenizer_kwargs = None
2021-07-11 13:34:09,629 - INFO - allennlp.common.params - dataset_reader.max_tokens = 512
2021-07-11 13:34:09,629 - INFO - allennlp.common.params - dataset_reader.text_num = 100
2021-07-11 13:34:09,630 - INFO - allennlp.common.params - train_data_path = data/BCUT/train.txt
2021-07-11 13:34:09,630 - INFO - allennlp.training.util - Reading training data from data/BCUT/train.txt
2021-07-11 13:34:09,630 - INFO - allennlp.common.params - data_loader.type = multiprocess
2021-07-11 13:34:09,630 - INFO - allennlp.common.params - data_loader.batch_size = 8
2021-07-11 13:34:09,630 - INFO - allennlp.common.params - data_loader.drop_last = False
2021-07-11 13:34:09,631 - INFO - allennlp.common.params - data_loader.shuffle = True
2021-07-11 13:34:09,631 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2021-07-11 13:34:09,631 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2021-07-11 13:34:09,631 - INFO - allennlp.common.params - data_loader.num_workers = 8
2021-07-11 13:34:09,631 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2021-07-11 13:34:09,631 - INFO - allennlp.common.params - data_loader.start_method = fork
2021-07-11 13:34:09,631 - INFO - allennlp.common.params - data_loader.cuda_device = None
2021-07-11 13:34:09,631 - INFO - allennlp.common.params - data_loader.quiet = False
2021-07-11 13:34:09,683 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2021-07-11 13:34:09,991 - INFO - allennlp.common.params - validation_dataset_reader = None
2021-07-11 13:34:09,991 - INFO - allennlp.common.params - validation_data_loader = data_loader.Params({'batch_size': 8, 'num_workers': 8, 'shuffle': True})
2021-07-11 13:34:09,991 - INFO - allennlp.common.params - validation_data_path = data/BCUT/test.txt
2021-07-11 13:34:09,992 - INFO - allennlp.training.util - Reading validation data from data/BCUT/test.txt
2021-07-11 13:34:09,992 - INFO - allennlp.common.params - data_loader.type = multiprocess
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.batch_size = 8
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.drop_last = False
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.shuffle = True
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.batch_sampler = None
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.num_workers = 8
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.start_method = fork
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.cuda_device = None
2021-07-11 13:34:09,993 - INFO - allennlp.common.params - data_loader.quiet = False
2021-07-11 13:34:10,063 - INFO - tqdm - loading instances: 0it [00:00, ?it/s]
2021-07-11 13:34:10,117 - INFO - allennlp.common.params - vocabulary.type = from_instances
2021-07-11 13:34:10,117 - INFO - allennlp.common.params - vocabulary.min_count = None
2021-07-11 13:34:10,118 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None
2021-07-11 13:34:10,118 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')
2021-07-11 13:34:10,118 - INFO - allennlp.common.params - vocabulary.pretrained_files = None
2021-07-11 13:34:10,118 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False
2021-07-11 13:34:10,118 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None
2021-07-11 13:34:10,119 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None
2021-07-11 13:34:10,119 - INFO - allennlp.common.params - vocabulary.padding_token = @@PADDING@@
2021-07-11 13:34:10,139 - INFO - allennlp.common.params - vocabulary.oov_token = @@UNKNOWN@@
2021-07-11 13:34:10,139 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2021-07-11 13:34:10,139 - INFO - tqdm - building vocab: 0it [00:00, ?it/s]
2021-07-11 13:34:10,144 - INFO - allennlp.training.util - writing the vocabulary to save1/vocabulary.
2021-07-11 13:34:10,186 - INFO - allennlp.training.util - done creating vocab
2021-07-11 13:34:10,189 - INFO - root - Switching to distributed training mode since multiple GPUs are configured | Primary is at: 127.0.0.1:60221 | Rank of this node: 0 | Number of workers in this node: 2 | Number of nodes: 1 | World size: 2
2021-07-11 13:34:43,818 - CRITICAL - root - Uncaught exception
Traceback (most recent call last):
  File "/usr/local/bin/allennlp", line 33, in <module>
    sys.exit(load_entry_point('allennlp', 'console_scripts', 'allennlp')())
  File "/stage/allennlp/allennlp/__main__.py", line 34, in run
    main(prog="allennlp")
  File "/stage/allennlp/allennlp/commands/__init__.py", line 119, in main
    args.func(args)
  File "/stage/allennlp/allennlp/commands/train.py", line 110, in train_model_from_args
    train_model_from_file(
  File "/stage/allennlp/allennlp/commands/train.py", line 170, in train_model_from_file
    return train_model(
  File "/stage/allennlp/allennlp/commands/train.py", line 308, in train_model
    mp.spawn(
  File "/usr/local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/usr/local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/usr/local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/stage/allennlp/allennlp/commands/train.py", line 453, in _train_worker
    train_loop = TrainModel.from_params(
  File "/stage/allennlp/allennlp/common/from_params.py", line 589, in from_params
    return retyped_subclass.from_params(
  File "/stage/allennlp/allennlp/common/from_params.py", line 623, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "/stage/allennlp/allennlp/commands/train.py", line 729, in from_partial_objects
    trainer_ = trainer.construct(
  File "/stage/allennlp/allennlp/common/lazy.py", line 80, in construct
    return self.constructor(**contructor_kwargs)
  File "/stage/allennlp/allennlp/common/lazy.py", line 64, in constructor_to_use
    return self._constructor.from_params(  # type: ignore[union-attr]
  File "/stage/allennlp/allennlp/common/from_params.py", line 589, in from_params
    return retyped_subclass.from_params(
  File "/stage/allennlp/allennlp/common/from_params.py", line 623, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "/stage/allennlp/allennlp/training/trainer.py", line 1070, in from_partial_objects
    return cls(
  File "/stage/allennlp/allennlp/training/trainer.py", line 361, in __init__
    self._pytorch_model = DistributedDataParallel(
  File "/usr/local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 446, in __init__
    self._sync_params_and_buffers(authoritative_rank=0)
  File "/usr/local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 457, in _sync_params_and_buffers
    self._distributed_broadcast_coalesced(
  File "/usr/local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1155, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled system error, NCCL version 2.7.8
ncclSystemError: System call (socket, malloc, munmap, etc) failed.

